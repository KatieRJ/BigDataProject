{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  RegExps (1 pts)\n",
    "\n",
    "-\tStrings with special syntax to help us to match pattenrs and find other strings\n",
    "-\tA pattern is a series of letters or symbols which can map to an actual text or words or punctuation. Usage: \n",
    "o\t find links in a webpage\n",
    "o\tparse email addresses \n",
    "o\tremove unwanted strings or characters. \n",
    "-\toften referred to as regex \n",
    "-\timport re \n",
    "-\tmethods: \n",
    "o\t re.match method\n",
    "\twhich matches a pattern with a string. \n",
    "o\tSplit\n",
    "o\tfindall, \n",
    "o\tsearch\n",
    "-\tThis can be used for tokenization, so you can preprocess text using regex while doing natural language processing.\n",
    "\n",
    "•  Bow  (1 pts)\n",
    "\n",
    "-\tThe BoW model is a way of representing text data when modeling text with machine learning algorithms.\n",
    "-\tIt simplifies operations with the text data we have \n",
    "-\tWe can count the words, segmentize them, lemmatize them \n",
    "-\tFor example: \n",
    "o\tbow_simple = Counter(lower_tokens)\n",
    "o\tfirst we need to make tokens from the text  text  words \n",
    "o\tand then count up all the words \n",
    "o\tthe more the word occurs the more importance it might have  can determine the significance of a word \n",
    "\n",
    "\n",
    "•  tokenization   (1 pts)\n",
    "-\tthe process of transforming a string or document into smaller chunks  tokens - like text into words - into an array. \n",
    "-\tThis is usually one step in the process of preparing a text for natural language processing. \n",
    "-\tThere are many different theories and rules regarding tokenization, \n",
    "-\tyou can create your own tokenization rules using regular expresssions, but normally tokenization will do things like break out words or sentences, often separate punctuation or you can even just tokenize parts of a string like separating all hashtags in a Tweet. \n",
    "-\tFrom nltk.tekonize import ….. \n",
    "-\tWhy bother with tokenization? \n",
    "o\tit can help us with some simple text processing tasks like \n",
    "\tmapping part of speech\n",
    "\tmatching common words \n",
    "\tperhaps removing unwanted tokens like common words or repeated words. Here, we have a good example. \n",
    "-\tOther tokenizers: \n",
    "o\tsent_tokenize\n",
    "o\tregexp_tokenize\n",
    "o\tTweetTokenizer\n",
    "\n",
    "•  lemmatization   (1 pts)\n",
    "\n",
    "-   from nltk.stem import WordNetLemmatizer\n",
    "-\twordnet_lemmatizer = WordNetLemmatizer()\n",
    "-\tnltk.download('wordnet')\n",
    "-\tnltk.download('omw-1.4')\n",
    "-\tlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "-\tlemmatized\n",
    "\n",
    "-\tit shows only the base of the word with no conjugation \n",
    "-\tprint(wordnet_lemmatizer.lemmatize(\"cats\")) --> cat\n",
    "\n",
    "•  tfidf   (1 pts)\n",
    "\n",
    "-\tterm-frequency - inverse document frequency. \n",
    "-\tIt is a commonly used natural language processing model \n",
    "-\thelps you determine the most important words in each document in the corpus. \n",
    "-\teach corpus might have more shared words than just stopwords. \n",
    "-\tTf-idf helps keep the document-specific frequent words weighted high and the common words across the entire corpus weighted low.\n",
    "-\tDifferent ways of doing it \n",
    "-\tWe could use \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "o\tFrom genism.models.tfidfmodel import TfidfModel\n",
    "ifidf = TfidfModel(corpus)\n",
    "\n",
    "\n",
    "•  Named entity recognition  (1 pts)\n",
    "-\tNLP method that extracts information from text\n",
    "-\tto identify important named entities in the text -- such as people, places and organizations -- they can even be dates, states, works of art and other categories depending on the libraries and notation you use. \n",
    "-\tcan be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?\n",
    "-\tNLTK allows you to interact with named entity recognition via it's own model, but also the aforementioned Stanford library. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
